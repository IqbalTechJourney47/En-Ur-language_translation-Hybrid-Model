{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b22e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\muhammad iqbal\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers[sentencepiece] sacrebleu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe0175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787e8e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Iqbal\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb5704a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Helsinki-NLP/opus-mt-en-ur'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b3912",
   "metadata": {},
   "source": [
    "# Helsinki-NLP/opus-mt-en-ur model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafb251",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/Helsinki-NLP/opus-mt-en-ur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5273ed",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d1517",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/datasets/cfilt/iitb-english-hindi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a201d",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/datasets/HaiderSultanArc/MT-Urdu-English/viewer/default/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7507d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('HaiderSultanArc/MT-Urdu-English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0a77c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ur'],\n",
       "        num_rows: 5646138\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'ur'],\n",
       "        num_rows: 1411535\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f29b795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'So, Are You In The Market?', 'ur': '\"تم یہاں بازار میں ؟\"'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98fd26",
   "metadata": {},
   "source": [
    "# Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "78bf5b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['en', 'ur'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['en', 'ur'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "raw_datasets = load_dataset('HaiderSultanArc/MT-Urdu-English')\n",
    "\n",
    "# Take a small subset for testing/debugging\n",
    "small_en_ur_train_dataset = raw_datasets['train'].shuffle(seed=42).select([i for i in range(1000)])  # Adjust the number as needed\n",
    "small_en_ur_test_dataset = raw_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # Adjust the number as needed\n",
    "\n",
    "# Display the small datasets\n",
    "small_en_ur_dataset = {\n",
    "    \"train\": small_en_ur_train_dataset,\n",
    "    \"test\": small_en_ur_test_dataset,\n",
    "}\n",
    "\n",
    "small_en_ur_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7078307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'And why are the kneeling on your right and on your left.',\n",
       " 'ur': '’’اور آپ ﷺ نے اپنی بائیں ہتھیلی کو بائیں ران اور گھٹنے پر رکھا۔‘‘'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_en_ur_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed086c",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3d530e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a116344",
   "metadata": {},
   "source": [
    "tokenizer(\"Hi, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4e9e3",
   "metadata": {},
   "source": [
    "tokenizer(\"Hi, this is a sentence!\", \"This is another sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b713334",
   "metadata": {},
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hi, this is a sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0d02a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ur\"\n",
    "\n",
    "# Define your preprocess_function as before\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[source_lang]\n",
    "    targets = examples[target_lang]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    # Add labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "296f86b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[57, 1082, 56, 3, 19626, 95, 76, 418, 7, 95, 76, 918, 5, 0], [1598, 108, 52, 82, 24319, 10376, 66, 179, 5, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[19977, 346, 88, 20669, 3292, 202, 3573, 752, 1016, 18, 132, 3032, 25058, 16, 3032, 23430, 4, 16061, 30, 12674, 17537, 0], [139, 20, 18, 257, 771, 2426, 1129, 11, 710, 1444, 8, 0]]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(small_en_ur_dataset[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6332e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocess_function to train and test datasets\n",
    "tokenized_train_datasets = small_en_ur_dataset[\"train\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ca86f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_datasets = small_en_ur_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "863f10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-ur.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3ccd2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5e8e6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49b4384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c119057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_train_datasets,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3c73659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_test_datasets,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7cecfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/62 [>.............................] - ETA: 37:51 - loss: 5.0565"
     ]
    }
   ],
   "source": [
    "model.fit(x=train_dataset, validation_data=test_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f72656c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"tf_model_ur/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e8a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce35952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
